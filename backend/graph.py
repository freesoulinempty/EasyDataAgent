import os                    
from dotenv import load_dotenv   
from langchain_openai import ChatOpenAI       
from langgraph.prebuilt import create_react_agent  
from langchain_core.tools import tool         
from pydantic import BaseModel, Field          
from langchain_tavily import TavilySearch     
import pandas as pd          
import pymysql              
import json                 
from datetime import datetime  
import matplotlib          
import matplotlib.pyplot as plt  
import seaborn as sns       
from reportlab.lib.pagesizes import letter          
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph  
from reportlab.lib.styles import getSampleStyleSheet 
from reportlab.lib import colors                     

# Load environment variables / åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(override=True)

# Create Tavily search tool / åˆ›å»ºTavilyæœç´¢å·¥å…·
search_tool = TavilySearch(max_results=5, topic="general")

# Create SQL query tool / åˆ›å»ºSQLæŸ¥è¯¢å·¥å…·
description = """
Call this function when users need to perform database queries.
This function runs SQL code on a specified MySQL server for data query tasks,
using pymysql to connect to the MySQL database.
This function only handles SQL code execution and data querying. For data extraction, use the extract_data function.
"""

# Define structured parameter model / å®šä¹‰ç»“æ„åŒ–å‚æ•°æ¨¡å‹
class SQLQuerySchema(BaseModel):
    sql_query: str = Field(description=description)

# ============================================================================
# SQL QUERY EXECUTION TOOL IMPLEMENTATION
# SQL æŸ¥è¯¢æ‰§è¡Œå·¥å…·å®ç°
# ============================================================================

@tool(args_schema=SQLQuerySchema)
def sql_inter(sql_query: str) -> str:
    """
    High-performance SQL query execution tool for database interaction
    é«˜æ€§èƒ½ SQL æŸ¥è¯¢æ‰§è¡Œå·¥å…·ï¼Œç”¨äºæ•°æ®åº“äº¤äº’
    
    This function serves as the primary interface for executing SQL queries against
    a MySQL database. It's designed for read-only operations and returns structured
    JSON data for further processing by other tools in the analysis pipeline.
    
    æ­¤å‡½æ•°ä½œä¸ºå¯¹ MySQL æ•°æ®åº“æ‰§è¡Œ SQL æŸ¥è¯¢çš„ä¸»è¦æ¥å£ã€‚
    å®ƒä¸“ä¸ºåªè¯»æ“ä½œè®¾è®¡ï¼Œè¿”å›ç»“æ„åŒ–çš„ JSON æ•°æ®ï¼Œ
    ä¾›åˆ†æç®¡é“ä¸­çš„å…¶ä»–å·¥å…·è¿›ä¸€æ­¥å¤„ç†ã€‚
    
    SECURITY FEATURES / å®‰å…¨ç‰¹æ€§:
    - Environment-based configuration (no hardcoded credentials)
    - åŸºäºç¯å¢ƒçš„é…ç½®ï¼ˆæ— ç¡¬ç¼–ç å‡­æ®ï¼‰
    - Automatic connection cleanup and resource management
    - è‡ªåŠ¨è¿æ¥æ¸…ç†å’Œèµ„æºç®¡ç†
    - UTF-8 encoding support for international data
    - UTF-8 ç¼–ç æ”¯æŒå›½é™…åŒ–æ•°æ®
    
    PERFORMANCE OPTIMIZATIONS / æ€§èƒ½ä¼˜åŒ–:
    - Connection pooling through environment configuration
    - é€šè¿‡ç¯å¢ƒé…ç½®çš„è¿æ¥æ± 
    - Efficient result formatting as JSON for downstream processing
    - é«˜æ•ˆçš„ç»“æœæ ¼å¼åŒ–ä¸º JSON ä¾›ä¸‹æ¸¸å¤„ç†
    - Minimal memory footprint with cursor-based operations
    - åŸºäºæ¸¸æ ‡æ“ä½œçš„æœ€å°å†…å­˜å ç”¨
    
    DATA FLOW / æ•°æ®æµ:
    SQL Query Input â†’ MySQL Execution â†’ Result Processing â†’ JSON Output
    SQL æŸ¥è¯¢è¾“å…¥ â†’ MySQL æ‰§è¡Œ â†’ ç»“æœå¤„ç† â†’ JSON è¾“å‡º
    
    :param sql_query: Well-formed SQL query string for database execution
                     ç”¨äºæ•°æ®åº“æ‰§è¡Œçš„è‰¯å¥½æ ¼å¼åŒ– SQL æŸ¥è¯¢å­—ç¬¦ä¸²
    :type sql_query: str
    
    :return: JSON-formatted query results or error message
             JSON æ ¼å¼çš„æŸ¥è¯¢ç»“æœæˆ–é”™è¯¯ä¿¡æ¯
    :rtype: str
    
    :raises: Connection errors, SQL syntax errors, timeout exceptions
             è¿æ¥é”™è¯¯ã€SQL è¯­æ³•é”™è¯¯ã€è¶…æ—¶å¼‚å¸¸
    
    Example Usage / ä½¿ç”¨ç¤ºä¾‹:
        result = sql_inter("SELECT * FROM customers LIMIT 10")
        # Returns: '[{"id": 1, "name": "John", ...}, ...]'
    """

    load_dotenv(override=True)
    
    host = os.getenv('HOST')           
    user = os.getenv('USER')           
    mysql_pw = os.getenv('MYSQL_PW')   
    db = os.getenv('DB_NAME')          
    port = os.getenv('MYSQL_PORT')           
    
    connection = pymysql.connect(
        host=host,                    
        user=user,                    
        passwd=mysql_pw,              
        db=db,                        
        port=int(port),               
        charset='utf8',               
        autocommit=True,              
        connect_timeout=30,           
        read_timeout=60               
    )
    
    # =======================================================================
    # SAFE SQL EXECUTION WITH RESOURCE MANAGEMENT
    # å®‰å…¨ SQL æ‰§è¡Œå’Œèµ„æºç®¡ç†
    # =======================================================================
    
    try:
        # Use context manager for automatic cursor cleanup
        # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è¿›è¡Œè‡ªåŠ¨æ¸¸æ ‡æ¸…ç†
        # This ensures proper resource disposal even on exceptions
        # è¿™ç¡®ä¿å³ä½¿åœ¨å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿèƒ½æ­£ç¡®é‡Šæ”¾èµ„æº
        with connection.cursor() as cursor:
            # Execute the SQL query with built-in error handling
            # æ‰§è¡Œ SQL æŸ¥è¯¢ï¼Œå†…ç½®é”™è¯¯å¤„ç†
            cursor.execute(sql_query)
            
            # Fetch all results efficiently into memory
            # é«˜æ•ˆåœ°å°†æ‰€æœ‰ç»“æœè·å–åˆ°å†…å­˜ä¸­
            # For large datasets, consider using fetchmany() for memory optimization
            # å¯¹äºå¤§å‹æ•°æ®é›†ï¼Œè€ƒè™‘ä½¿ç”¨ fetchmany() è¿›è¡Œå†…å­˜ä¼˜åŒ–
            results = cursor.fetchall()
            
            # Optional success logging - useful for debugging
            # å¯é€‰çš„æˆåŠŸæ—¥å¿— - ç”¨äºè°ƒè¯•å¾ˆæœ‰ç”¨
            # print("SQL query executed successfully, organizing results... / SQL æŸ¥è¯¢å·²æˆåŠŸæ‰§è¡Œï¼Œæ­£åœ¨æ•´ç†ç»“æœ...")
            
    except pymysql.Error as e:
        # Handle MySQL-specific errors with detailed information
        # å¤„ç† MySQL ç‰¹å®šé”™è¯¯ï¼Œæä¾›è¯¦ç»†ä¿¡æ¯
        error_msg = f"MySQL Error {e.args[0]}: {e.args[1]}"
        return json.dumps({"error": error_msg, "query": sql_query}, ensure_ascii=False)
    
    except Exception as e:
        # Handle general exceptions with context
        # å¤„ç†ä¸€èˆ¬å¼‚å¸¸ï¼Œæä¾›ä¸Šä¸‹æ–‡
        error_msg = f"Query execution failed: {str(e)}"
        return json.dumps({"error": error_msg, "query": sql_query}, ensure_ascii=False)
    
    finally:
        # Ensure connection is always closed to prevent resource leaks
        # ç¡®ä¿è¿æ¥å§‹ç»ˆå…³é—­ï¼Œé˜²æ­¢èµ„æºæ³„æ¼
        # This runs regardless of success or failure
        # æ— è®ºæˆåŠŸæˆ–å¤±è´¥éƒ½ä¼šè¿è¡Œ
        if 'connection' in locals() and connection.open:
            connection.close()

    return json.dumps(
        results, 
        ensure_ascii=False,   
        default=str,           
        indent=2                
    )

# ============================================================================
# DATA EXTRACTION TOOL CONFIGURATION
# æ•°æ®æå–å·¥å…·é…ç½®
# ============================================================================
# This tool differs from sql_inter by importing data directly into the Python
# environment as pandas DataFrames for immediate analysis and manipulation.
# æ­¤å·¥å…·ä¸ sql_inter ä¸åŒï¼Œå®ƒå°†æ•°æ®ç›´æ¥å¯¼å…¥ Python ç¯å¢ƒ
# ä½œä¸º pandas DataFramesï¼Œä¾›ç«‹å³åˆ†æå’Œæ“ä½œã€‚
# ============================================================================

# Pydantic schema for data extraction tool input validation
# æ•°æ®æå–å·¥å…·è¾“å…¥éªŒè¯çš„ Pydantic æ¨¡å¼
class ExtractQuerySchema(BaseModel):
    """Schema for validating data extraction parameters | æ•°æ®æå–å‚æ•°éªŒè¯æ¨¡å¼"""
    
    sql_query: str = Field(
        description="SQL query statement for extracting data from MySQL database into Python environment / ç”¨äºä» MySQL æ•°æ®åº“æå–æ•°æ®åˆ° Python ç¯å¢ƒçš„ SQL æŸ¥è¯¢è¯­å¥",
        min_length=1,
        max_length=10000
    )
    
    df_name: str = Field(
        description="Variable name for storing the extracted DataFrame in global scope (must be valid Python identifier) / ç”¨äºåœ¨å…¨å±€ä½œç”¨åŸŸä¸­å­˜å‚¨æå–çš„ DataFrame çš„å˜é‡åï¼ˆå¿…é¡»æ˜¯æœ‰æ•ˆçš„ Python æ ‡è¯†ç¬¦ï¼‰",
        min_length=1,
        max_length=100,
        pattern=r'^[a-zA-Z_][a-zA-Z0-9_]*$'  # Valid Python identifier / æœ‰æ•ˆçš„ Python æ ‡è¯†ç¬¦
    )

# ============================================================================
# DATA EXTRACTION TOOL IMPLEMENTATION
# æ•°æ®æå–å·¥å…·å®ç°
# ============================================================================
@tool(args_schema=ExtractQuerySchema)
def extract_data(sql_query: str, df_name: str) -> str:
    """
    Extract a table from MySQL database to the current Python environment. Note that this function only handles data extraction,
    not data querying. For data queries in MySQL, use the sql_inter function.
    Also note that when writing external function parameter messages, they must be strings in JSON format.
    
    :param sql_query: SQL query statement in string format for extracting a table from MySQL
    :param df_name: Variable name for locally saving the table extracted from MySQL database, represented as a string
    :return: Table reading and saving results
    """
    # Active status logging for data extraction operations
    # æ•°æ®æå–æ“ä½œçš„æ´»åŠ¨çŠ¶æ€æ—¥å¿—
    print("Calling extract_data tool to run SQL query... / æ­£åœ¨è°ƒç”¨ extract_data å·¥å…·è¿è¡Œ SQL æŸ¥è¯¢...")
    
    load_dotenv(override=True)
    host = os.getenv('HOST')
    user = os.getenv('USER')
    mysql_pw = os.getenv('MYSQL_PW')
    db = os.getenv('DB_NAME')
    port = os.getenv('MYSQL_PORT')

    # Create database connection / åˆ›å»ºæ•°æ®åº“è¿æ¥
    connection = pymysql.connect(
        host=host,
        user=user,
        passwd=mysql_pw,
        db=db,
        port=int(port),
        charset='utf8'
    )

    try:
        # Execute SQL and save as global variable / æ‰§è¡Œ SQL å¹¶ä¿å­˜ä¸ºå…¨å±€å˜é‡
        df = pd.read_sql(sql_query, connection)
        globals()[df_name] = df
        # Optional success confirmation - useful for development
        # å¯é€‰çš„æˆåŠŸç¡®è®¤ - ç”¨äºå¼€å‘å¾ˆæœ‰ç”¨
        # print("Data successfully extracted and saved as global variable: / æ•°æ®æˆåŠŸæå–å¹¶ä¿å­˜ä¸ºå…¨å±€å˜é‡ï¼š", df_name)
        return f"Successfully created pandas object `{df_name}` containing data extracted from MySQL."
    except Exception as e:
        return f"Execution failed: {e}"
    finally:
        connection.close()

# Create Python code execution tool / åˆ›å»ºPythonä»£ç æ‰§è¡Œå·¥å…·
# Python code execution tool structured parameter description / Pythonä»£ç æ‰§è¡Œå·¥å…·ç»“æ„åŒ–å‚æ•°è¯´æ˜
class PythonCodeInput(BaseModel):
    py_code: str = Field(description="A valid Python code string, e.g., '2 + 2' or 'x = 3\\ny = x * 2'")

@tool(args_schema=PythonCodeInput)
def python_inter(py_code):
    """
    Call this function when users need to write and execute Python programs.
    This function can execute Python code and return the final result. Note that this function can only execute non-plotting code.
    For plotting-related code, use the fig_inter function.
    """    
    g = globals()
    try:
        # Try to return expression result if it's an expression / å°è¯•å¦‚æœæ˜¯è¡¨è¾¾å¼ï¼Œåˆ™è¿”å›è¡¨è¾¾å¼è¿è¡Œç»“æœ
        return str(eval(py_code, g))
    # If error occurs, test if it's repeated assignment to the same variable / è‹¥æŠ¥é”™ï¼Œåˆ™å…ˆæµ‹è¯•æ˜¯å¦æ˜¯å¯¹ç›¸åŒå˜é‡é‡å¤èµ‹å€¼
    except Exception as e:
        global_vars_before = set(g.keys())
        try:            
            exec(py_code, g)
        except Exception as e:
            return f"Code execution error: {e}"
        global_vars_after = set(g.keys())
        new_vars = global_vars_after - global_vars_before
        # If new variables exist / è‹¥å­˜åœ¨æ–°å˜é‡
        if new_vars:
            result = {var: g[var] for var in new_vars}
            # Optional execution confirmation for debugging
            # å¯é€‰çš„æ‰§è¡Œç¡®è®¤ç”¨äºè°ƒè¯•
            # print("ä»£ç å·²é¡ºåˆ©æ‰§è¡Œï¼Œæ­£åœ¨è¿›è¡Œç»“æœæ¢³ç†...")
            return str(result)
        else:
            # Optional execution confirmation for debugging
            # å¯é€‰çš„æ‰§è¡Œç¡®è®¤ç”¨äºè°ƒè¯•
            # print("ä»£ç å·²é¡ºåˆ©æ‰§è¡Œï¼Œæ­£åœ¨è¿›è¡Œç»“æœæ¢³ç†...")
            return "Code executed successfully"

# Create plotting tool / åˆ›å»ºç»˜å›¾å·¥å…·
# Plotting tool structured parameter description / ç»˜å›¾å·¥å…·ç»“æ„åŒ–å‚æ•°è¯´æ˜
class FigCodeInput(BaseModel):
    py_code: str = Field(description="Python plotting code to execute, must use matplotlib/seaborn to create images and assign to variables")
    fname: str = Field(description="Variable name of the image object, e.g., 'fig', used to extract from code and save as image")

@tool(args_schema=FigCodeInput)
def fig_inter(py_code: str, fname: str) -> str:
    """
    Call this function when users need to use Python for visualization plotting tasks.

    Notes:
    1. All plotting code must create an image object and assign it to the specified variable name (e.g., `fig`).
    2. Must use `fig = plt.figure()` or `fig = plt.subplots()`.
    3. Do not use `plt.show()`.
    4. Ensure the code ends with `fig.tight_layout()`.
    5. All text content in plotting code including axis labels (xlabel, ylabel), titles, legends must be in English.

    Example code:
    fig = plt.figure(figsize=(10,6))
    plt.plot([1,2,3], [4,5,6])
    fig.tight_layout()
    """
    # Optional debug output for monitoring tool usage
    # å¯é€‰çš„è°ƒè¯•è¾“å‡ºç”¨äºç›‘æ§å·¥å…·ä½¿ç”¨
    # print("Calling fig_inter tool to run Python code... / æ­£åœ¨è°ƒç”¨fig_interå·¥å…·è¿è¡ŒPythonä»£ç ...")

    current_backend = matplotlib.get_backend()
    matplotlib.use('Agg')

    local_vars = {"plt": plt, "pd": pd, "sns": sns}
    
    # Set image save path (from environment variable) / è®¾ç½®å›¾åƒä¿å­˜è·¯å¾„ï¼ˆä»ç¯å¢ƒå˜é‡ï¼‰
    base_dir = os.getenv('PUBLIC_DIR', "/app/shared/public")
    images_dir = os.path.join(base_dir, "images")
    os.makedirs(images_dir, exist_ok=True)  # Automatically create images folder if it doesn't exist / è‡ªåŠ¨åˆ›å»º images æ–‡ä»¶å¤¹ï¼ˆå¦‚ä¸å­˜åœ¨ï¼‰
    try:
        g = globals()
        exec(py_code, g, local_vars)
        g.update(local_vars)

        fig = local_vars.get(fname, None)
        if fig:
            image_filename = f"{fname}.png"
            abs_path = os.path.join(images_dir, image_filename)  # Absolute path / ç»å¯¹è·¯å¾„
            rel_path = os.path.join("images", image_filename)    # Return relative path (for frontend) / è¿”å›ç›¸å¯¹è·¯å¾„ï¼ˆç»™å‰ç«¯ç”¨ï¼‰

            fig.savefig(abs_path, bbox_inches='tight')
            return f"Image saved successfully: {rel_path}\n\n---\n\n![Generated Chart]({rel_path})"
        else:
            return "Image object not found, please confirm the variable name is correct and is a matplotlib figure object."
    except Exception as e:
        return f"Execution failed: {e}"
    finally:
        plt.close('all')
        matplotlib.use(current_backend)

# Load prompt from external file / ä»å¤–éƒ¨æ–‡ä»¶åŠ è½½æç¤ºè¯
def load_prompt():
    try:
        with open('prompt.txt', 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        # Fallback prompt if file not found
        return """
        You are EasyDataAgent, a professional data analysis consultant with comprehensive analytical capabilities.
        Use the available tools to help users with data analysis, visualization, and insights.
        Always be proactive in suggesting relevant tools and providing professional guidance.
        """

prompt = load_prompt()

# ============================================================================
# MULTI-FORMAT DATA EXPORT TOOL CONFIGURATION
# å¤šæ ¼å¼æ•°æ®å¯¼å‡ºå·¥å…·é…ç½®
# ============================================================================
# This tool provides enterprise-grade data export capabilities for sharing analysis results
# è¯¥å·¥å…·æä¾›ä¼ä¸šçº§æ•°æ®å¯¼å‡ºåŠŸèƒ½ï¼Œç”¨äºåˆ†äº«åˆ†æç»“æœ
# Supports Excel (business users), JSON (technical users), PDF (executive reports)
# æ”¯æŒExcelï¼ˆä¸šåŠ¡ç”¨æˆ·ï¼‰ã€JSONï¼ˆæŠ€æœ¯ç”¨æˆ·ï¼‰ã€PDFï¼ˆæ‰§è¡ŒæŠ¥å‘Šï¼‰

# Data export schema definition / æ•°æ®å¯¼å‡ºæ¨¡å¼å®šä¹‰
class DataExportSchema(BaseModel):
    """
    Input validation schema for data export operations
    æ•°æ®å¯¼å‡ºæ“ä½œçš„è¾“å…¥éªŒè¯æ¨¡å¼
    
    Ensures proper parameter types and format validation for export operations
    ç¡®ä¿å¯¼å‡ºæ“ä½œçš„å‚æ•°ç±»å‹å’Œæ ¼å¼éªŒè¯æ­£ç¡®
    """
    df_name: str = Field(description="Name of the pandas DataFrame variable to export / è¦å¯¼å‡ºçš„pandas DataFrameå˜é‡å")
    format_type: str = Field(description="Export format: 'excel', 'json', or 'pdf' / å¯¼å‡ºæ ¼å¼ï¼š'excel'ã€'json'æˆ–'pdf'")
    filename: str = Field(description="Output filename (without extension) / è¾“å‡ºæ–‡ä»¶åï¼ˆä¸åŒ…å«æ‰©å±•åï¼‰")

@tool(args_schema=DataExportSchema)
def export_data(df_name: str, format_type: str, filename: str) -> str:
    """
    MULTI-FORMAT DATA EXPORT FUNCTION
    å¤šæ ¼å¼æ•°æ®å¯¼å‡ºåŠŸèƒ½
    
    Export pandas DataFrame to various professional formats for different stakeholder needs.
    å°†pandas DataFrameå¯¼å‡ºä¸ºå„ç§ä¸“ä¸šæ ¼å¼ï¼Œæ»¡è¶³ä¸åŒåˆ©ç›Šç›¸å…³è€…çš„éœ€æ±‚ã€‚
    
    BUSINESS VALUE / ä¸šåŠ¡ä»·å€¼:
    - Professional reporting for executives / ä¸ºç®¡ç†å±‚æä¾›ä¸“ä¸šæŠ¥å‘Š
    - Data sharing across different platforms / è·¨å¹³å°æ•°æ®å…±äº«
    - Archive analysis results for future reference / å½’æ¡£åˆ†æç»“æœä¾›æœªæ¥å‚è€ƒ
    - Integration with external systems / ä¸å¤–éƒ¨ç³»ç»Ÿé›†æˆ
    
    EXPORT FORMATS & USE CASES / å¯¼å‡ºæ ¼å¼å’Œä½¿ç”¨åœºæ™¯:
    1. Excel (.xlsx): Business users, data manipulation, pivot tables / ä¸šåŠ¡ç”¨æˆ·ã€æ•°æ®æ“ä½œã€æ•°æ®é€è§†è¡¨
    2. JSON (.json): API integration, web applications, data exchange / APIé›†æˆã€Webåº”ç”¨ã€æ•°æ®äº¤æ¢
    3. PDF (.pdf): Executive reports, presentations, documentation / æ‰§è¡ŒæŠ¥å‘Šã€æ¼”ç¤ºã€æ–‡æ¡£
    
    WORKFLOW PROCESS / å·¥ä½œæµç¨‹:
    Step 1: Validate DataFrame existence and type / æ­¥éª¤1ï¼šéªŒè¯DataFrameå­˜åœ¨æ€§å’Œç±»å‹
    Step 2: Create export directory structure / æ­¥éª¤2ï¼šåˆ›å»ºå¯¼å‡ºç›®å½•ç»“æ„
    Step 3: Execute format-specific export logic / æ­¥éª¤3ï¼šæ‰§è¡Œç‰¹å®šæ ¼å¼çš„å¯¼å‡ºé€»è¾‘
    Step 4: Return success status and file path / æ­¥éª¤4ï¼šè¿”å›æˆåŠŸçŠ¶æ€å’Œæ–‡ä»¶è·¯å¾„
    
    :param df_name: Name of the pandas DataFrame variable to export / è¦å¯¼å‡ºçš„pandas DataFrameå˜é‡å
    :param format_type: Export format - 'excel', 'json', or 'pdf' / å¯¼å‡ºæ ¼å¼ - 'excel'ã€'json'æˆ–'pdf'
    :param filename: Output filename without extension / ä¸åŒ…å«æ‰©å±•åçš„è¾“å‡ºæ–‡ä»¶å
    :return: Export status and relative file path for web access / å¯¼å‡ºçŠ¶æ€å’Œç”¨äºWebè®¿é—®çš„ç›¸å¯¹æ–‡ä»¶è·¯å¾„
    """
    try:
        # ========================================================================
        # STEP 1: DATAFRAME VALIDATION AND RETRIEVAL
        # æ­¥éª¤1ï¼šDataFrameéªŒè¯å’Œè·å–
        # ========================================================================
        
        # Retrieve DataFrame from global namespace (injected by extract_data or python_inter)
        # ä»å…¨å±€å‘½åç©ºé—´è·å–DataFrameï¼ˆç”±extract_dataæˆ–python_interæ³¨å…¥ï¼‰
        g = globals()
        if df_name not in g:
            return f"Error: DataFrame '{df_name}' not found. Please extract or create the DataFrame first."
        
        # Ensure the object is actually a pandas DataFrame
        # ç¡®ä¿å¯¹è±¡ç¡®å®æ˜¯pandas DataFrame
        df = g[df_name]
        if not isinstance(df, pd.DataFrame):
            return f"Error: '{df_name}' is not a pandas DataFrame."
        
        # ========================================================================
        # STEP 2: EXPORT DIRECTORY SETUP AND FILE PATH MANAGEMENT
        # æ­¥éª¤2ï¼šå¯¼å‡ºç›®å½•è®¾ç½®å’Œæ–‡ä»¶è·¯å¾„ç®¡ç†
        # ========================================================================
        
        # Define base directory for web-accessible exports
        # å®šä¹‰ç”¨äºWebå¯è®¿é—®å¯¼å‡ºçš„åŸºæœ¬ç›®å½•
        # This path allows the web UI to access exported files
        # æ­¤è·¯å¾„å…è®¸Web UIè®¿é—®å¯¼å‡ºçš„æ–‡ä»¶
        base_dir = os.getenv('PUBLIC_DIR', "/app/shared/public")
        exports_dir = os.path.join(base_dir, "exports")
        
        # Create exports directory if it doesn't exist
        # å¦‚æœå¯¼å‡ºç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»º
        os.makedirs(exports_dir, exist_ok=True)
        
        # ========================================================================
        # STEP 3A: EXCEL FORMAT EXPORT PROCESSING
        # æ­¥éª¤3Aï¼šExcelæ ¼å¼å¯¼å‡ºå¤„ç†
        # ========================================================================
        
        if format_type.lower() == 'excel':
            # Excel export for business users and data manipulation
            # é¢å‘ä¸šåŠ¡ç”¨æˆ·å’Œæ•°æ®æ“ä½œçš„Excelå¯¼å‡º
            file_path = os.path.join(exports_dir, f"{filename}.xlsx")
            
            # Export DataFrame to Excel with index for row identification
            # å°†DataFrameå¯¼å‡ºä¸ºExcelï¼ŒåŒ…å«ç´¢å¼•ç”¨äºè¡Œè¯†åˆ«
            # openpyxl engine provides robust Excel compatibility
            # openpyxlå¼•æ“æä¾›å¼ºå¤§çš„Excelå…¼å®¹æ€§
            df.to_excel(file_path, index=True, engine='openpyxl')
            
            # Return relative path for web UI access
            # è¿”å›ç”¨äºWeb UIè®¿é—®çš„ç›¸å¯¹è·¯å¾„
            rel_path = os.path.join("exports", f"{filename}.xlsx")
            return f"Excel file exported successfully: {rel_path}"
            
        # ========================================================================
        # STEP 3B: JSON FORMAT EXPORT PROCESSING
        # æ­¥éª¤3Bï¼šJSONæ ¼å¼å¯¼å‡ºå¤„ç†
        # ========================================================================
        
        elif format_type.lower() == 'json':
            # JSON export for API integration and web applications
            # é¢å‘APIé›†æˆå’ŒWebåº”ç”¨çš„JSONå¯¼å‡º
            file_path = os.path.join(exports_dir, f"{filename}.json")
            
            # Export DataFrame as JSON with records orientation
            # ä»¥recordsæ–¹å‘å°†DataFrameå¯¼å‡ºä¸ºJSON
            # 'records' format: [{col1: val1, col2: val2}, ...] - most API-friendly
            # 'records'æ ¼å¼ï¼š[{col1: val1, col2: val2}, ...] - æœ€é€‚åˆAPI
            # ISO date format ensures international compatibility
            # ISOæ—¥æœŸæ ¼å¼ç¡®ä¿å›½é™…å…¼å®¹æ€§
            # Pretty printing with indent=2 for readability
            # ä½¿ç”¨indent=2è¿›è¡Œç¾åŒ–æ‰“å°ä»¥æé«˜å¯è¯»æ€§
            df.to_json(file_path, orient='records', date_format='iso', indent=2)
            
            # Return relative path for web UI access
            # è¿”å›ç”¨äºWeb UIè®¿é—®çš„ç›¸å¯¹è·¯å¾„
            rel_path = os.path.join("exports", f"{filename}.json")
            return f"JSON file exported successfully: {rel_path}"
            
        # ========================================================================
        # STEP 3C: PDF FORMAT EXPORT PROCESSING
        # æ­¥éª¤3Cï¼šPDFæ ¼å¼å¯¼å‡ºå¤„ç†
        # ========================================================================
        
        elif format_type.lower() == 'pdf':
            # PDF export for executive reports and presentations
            # é¢å‘æ‰§è¡ŒæŠ¥å‘Šå’Œæ¼”ç¤ºçš„PDFå¯¼å‡º
            file_path = os.path.join(exports_dir, f"{filename}.pdf")
            
            # ================================================================
            # PDF DOCUMENT STRUCTURE INITIALIZATION
            # PDFæ–‡æ¡£ç»“æ„åˆå§‹åŒ–
            # ================================================================
            # Create professional PDF document with standard letter size
            # åˆ›å»ºæ ‡å‡†ä¿¡çº¸å°ºå¯¸çš„ä¸“ä¸šPDFæ–‡æ¡£
            doc = SimpleDocTemplate(file_path, pagesize=letter)
            elements = []  # Document elements container / æ–‡æ¡£å…ƒç´ å®¹å™¨
            styles = getSampleStyleSheet()  # Professional styling / ä¸“ä¸šæ ·å¼
            
            # ================================================================
            # DOCUMENT HEADER AND METADATA
            # æ–‡æ¡£æ ‡é¢˜å’Œå…ƒæ•°æ®
            # ================================================================
            # Add professional title for the data export
            # ä¸ºæ•°æ®å¯¼å‡ºæ·»åŠ ä¸“ä¸šæ ‡é¢˜
            title = Paragraph(f"Data Export: {df_name}", styles['Title'])
            elements.append(title)
            
            # Add timestamp for report tracking and versioning
            # æ·»åŠ æ—¶é—´æˆ³ç”¨äºæŠ¥å‘Šè·Ÿè¸ªå’Œç‰ˆæœ¬ç®¡ç†
            timestamp = Paragraph(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", styles['Normal'])
            elements.append(timestamp)
            
            # ================================================================
            # DATA TABLE PREPARATION AND FORMATTING
            # æ•°æ®è¡¨æ ¼å‡†å¤‡å’Œæ ¼å¼åŒ–
            # ================================================================
            # Convert DataFrame to table format with headers
            # å°†DataFrameè½¬æ¢ä¸ºå¸¦æœ‰æ ‡é¢˜çš„è¡¨æ ¼æ ¼å¼
            data = [df.columns.tolist()]  # Start with column headers / ä»¥åˆ—æ ‡é¢˜å¼€å§‹
            
            # Limit rows for PDF readability and performance
            # é™åˆ¶è¡Œæ•°ä»¥æé«˜PDFå¯è¯»æ€§å’Œæ€§èƒ½
            max_rows = min(50, len(df))  # Maximum 50 rows for optimal PDF rendering / æœ€å¤š50è¡Œä»¥ä¼˜åŒ–PDFæ¸²æŸ“
            
            # Convert DataFrame rows to string format for PDF compatibility
            # å°†DataFrameè¡Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ä»¥å…¼å®¹PDF
            for _, row in df.head(max_rows).iterrows():
                data.append([str(x) for x in row.tolist()])
            
            # ================================================================
            # PROFESSIONAL TABLE STYLING
            # ä¸“ä¸šè¡¨æ ¼æ ·å¼
            # ================================================================
            # Create formatted table with professional styling
            # åˆ›å»ºå…·æœ‰ä¸“ä¸šæ ·å¼çš„æ ¼å¼åŒ–è¡¨æ ¼
            table = Table(data)
            table.setStyle(TableStyle([
                # Header row styling / æ ‡é¢˜è¡Œæ ·å¼
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),        # Header background / æ ‡é¢˜èƒŒæ™¯
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),   # Header text color / æ ‡é¢˜æ–‡æœ¬é¢œè‰²
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),              # Center alignment / å±…ä¸­å¯¹é½
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),    # Bold header font / ç²—ä½“æ ‡é¢˜å­—ä½“
                ('FONTSIZE', (0, 0), (-1, 0), 12),                  # Header font size / æ ‡é¢˜å­—ä½“å¤§å°
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),             # Header padding / æ ‡é¢˜å†…è¾¹è·
                # Data rows styling / æ•°æ®è¡Œæ ·å¼
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),     # Data background / æ•°æ®èƒŒæ™¯
                ('GRID', (0, 0), (-1, -1), 1, colors.black)         # Table grid / è¡¨æ ¼ç½‘æ ¼
            ]))
            
            elements.append(table)
            
            # ================================================================
            # PAGINATION NOTICE FOR LARGE DATASETS
            # å¤§å‹æ•°æ®é›†çš„åˆ†é¡µé€šçŸ¥
            # ================================================================
            # Add note if data was truncated for PDF optimization
            # å¦‚æœä¸ºäº†PDFä¼˜åŒ–è€Œæˆªæ–­æ•°æ®ï¼Œåˆ™æ·»åŠ è¯´æ˜
            if len(df) > max_rows:
                note = Paragraph(f"Note: Only first {max_rows} rows shown. Total rows: {len(df)}", styles['Normal'])
                elements.append(note)
            
            # Build the PDF document with all elements
            # ä½¿ç”¨æ‰€æœ‰å…ƒç´ æ„å»ºPDFæ–‡æ¡£
            doc.build(elements)
            
            # Return relative path for web UI access
            # è¿”å›ç”¨äºWeb UIè®¿é—®çš„ç›¸å¯¹è·¯å¾„
            rel_path = os.path.join("exports", f"{filename}.pdf")
            return f"PDF file exported successfully: {rel_path}"
            
        # ========================================================================
        # STEP 3D: UNSUPPORTED FORMAT HANDLING
        # æ­¥éª¤3Dï¼šä¸æ”¯æŒæ ¼å¼å¤„ç†
        # ========================================================================
        
        else:
            # Handle unsupported export formats gracefully
            # ä¼˜é›…åœ°å¤„ç†ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼
            return f"Error: Unsupported format '{format_type}'. Supported formats: excel, json, pdf"
            
    except Exception as e:
        # ========================================================================
        # COMPREHENSIVE ERROR HANDLING AND RECOVERY
        # ç»¼åˆé”™è¯¯å¤„ç†å’Œæ¢å¤
        # ========================================================================
        # Catch and report any unexpected errors during export process
        # æ•è·å¹¶æŠ¥å‘Šå¯¼å‡ºè¿‡ç¨‹ä¸­çš„ä»»ä½•æ„å¤–é”™è¯¯
        return f"Export failed: {str(e)}"

# ============================================================================
# COMPREHENSIVE DATA PREVIEW TOOL CONFIGURATION
# ç»¼åˆæ•°æ®é¢„è§ˆå·¥å…·é…ç½®
# ============================================================================
# Advanced data exploration and quality assessment capabilities
# é«˜çº§æ•°æ®æ¢ç´¢å’Œè´¨é‡è¯„ä¼°åŠŸèƒ½
# ============================================================================

class DataPreviewSchema(BaseModel):
    """Schema for data preview tool input validation | æ•°æ®é¢„è§ˆå·¥å…·è¾“å…¥éªŒè¯æ¨¡å¼"""
    
    df_name: str = Field(
        description="Name of the pandas DataFrame variable for comprehensive preview / ç”¨äºç»¼åˆé¢„è§ˆçš„ pandas DataFrame å˜é‡å",
        min_length=1,
        max_length=100,
        pattern=r'^[a-zA-Z_][a-zA-Z0-9_]*$'  # Valid Python identifier / æœ‰æ•ˆçš„ Python æ ‡è¯†ç¬¦
    )
    
    rows: int = Field(
        default=10,
        description="Number of sample rows to display for data preview (1-100) / ç”¨äºæ•°æ®é¢„è§ˆæ˜¾ç¤ºçš„æ ·æœ¬è¡Œæ•° (1-100)",
        ge=1,      # Minimum 1 row / æœ€å°‘ 1 è¡Œ
        le=100     # Maximum 100 rows for performance / æœ€å¤š 100 è¡Œä»¥ä¿è¯æ€§èƒ½
    )

@tool(args_schema=DataPreviewSchema)
def data_preview(df_name: str, rows: int = 10) -> str:
    """
    Enterprise-grade data preview and exploration tool for comprehensive dataset analysis
    ä¼ä¸šçº§æ•°æ®é¢„è§ˆå’Œæ¢ç´¢å·¥å…·ï¼Œç”¨äºç»¼åˆæ•°æ®é›†åˆ†æ
    
    This function provides a detailed, multi-dimensional analysis of DataFrames,
    combining statistical insights, data quality metrics, and structural information
    into a comprehensive report suitable for both technical analysis and business
    stakeholder communication.
    
    æ­¤å‡½æ•°æä¾› DataFrame çš„è¯¦ç»†ã€å¤šç»´åº¦åˆ†æï¼Œ
    å°†ç»Ÿè®¡æ´å¯Ÿã€æ•°æ®è´¨é‡æŒ‡æ ‡å’Œç»“æ„ä¿¡æ¯ç»“åˆæˆ
    é€‚ç”¨äºæŠ€æœ¯åˆ†æå’Œä¸šåŠ¡åˆ©ç›Šç›¸å…³è€…æ²Ÿé€šçš„ç»¼åˆæŠ¥å‘Šã€‚
    
    COMPREHENSIVE ANALYSIS FEATURES / ç»¼åˆåˆ†æåŠŸèƒ½:
    - Dataset structure and dimensionality analysis
    - æ•°æ®é›†ç»“æ„å’Œç»´åº¦åˆ†æ
    - Memory usage optimization recommendations
    - å†…å­˜ä½¿ç”¨ä¼˜åŒ–å»ºè®®
    - Data type analysis with conversion suggestions
    - æ•°æ®ç±»å‹åˆ†æåŠè½¬æ¢å»ºè®®
    - Missing value patterns and impact assessment
    - ç¼ºå¤±å€¼æ¨¡å¼å’Œå½±å“è¯„ä¼°
    - Statistical summaries for numeric and categorical data
    - æ•°å€¼å’Œåˆ†ç±»æ•°æ®çš„ç»Ÿè®¡æ‘˜è¦
    - Sample data with intelligent row selection
    - å…·æœ‰æ™ºèƒ½è¡Œé€‰æ‹©çš„æ ·æœ¬æ•°æ®
    
    BUSINESS INTELLIGENCE INSIGHTS / å•†ä¸šæ™ºèƒ½æ´å¯Ÿ:
    - Data readiness assessment for analysis
    - åˆ†ææ•°æ®å°±ç»ªè¯„ä¼°
    - Quality indicators and recommendations
    - è´¨é‡æŒ‡æ ‡å’Œå»ºè®®
    - Performance optimization opportunities
    - æ€§èƒ½ä¼˜åŒ–æœºä¼š
    
    TECHNICAL SPECIFICATIONS / æŠ€æœ¯è§„æ ¼:
    - Unicode and emoji support in output
    - è¾“å‡ºä¸­çš„ Unicode å’Œè¡¨æƒ…ç¬¦å·æ”¯æŒ
    - Professional formatting with visual hierarchy
    - å…·æœ‰è§†è§‰å±‚æ¬¡ç»“æ„çš„ä¸“ä¸šæ ¼å¼åŒ–
    - Timestamp tracking for audit trails
    - ç”¨äºå®¡è®¡è·Ÿè¸ªçš„æ—¶é—´æˆ³è®°å½•
    
    :param df_name: Name of DataFrame variable in global scope for analysis
                   å…¨å±€ä½œç”¨åŸŸä¸­ç”¨äºåˆ†æçš„ DataFrame å˜é‡å
    :type df_name: str
    
    :param rows: Number of representative sample rows to display
                è¦æ˜¾ç¤ºçš„ä»£è¡¨æ€§æ ·æœ¬è¡Œæ•°
    :type rows: int
    
    :return: Comprehensive formatted data preview report
             ç»¼åˆæ ¼å¼åŒ–æ•°æ®é¢„è§ˆæŠ¥å‘Š
    :rtype: str
    
    :raises: TypeError if df_name is not a DataFrame, KeyError if variable not found
             å¦‚æœ df_name ä¸æ˜¯ DataFrame åˆ™å¼•å‘ TypeErrorï¼Œå¦‚æœæ‰¾ä¸åˆ°å˜é‡åˆ™å¼•å‘ KeyError
    
    Example Usage / ä½¿ç”¨ç¤ºä¾‹:
        data_preview("sales_data", 15)
        # Returns comprehensive preview with 15 sample rows
        # è¿”å›åŒ…å« 15 ä¸ªæ ·æœ¬è¡Œçš„ç»¼åˆé¢„è§ˆ
    """
    try:
        # Get DataFrame from global variables
        g = globals()
        if df_name not in g:
            return f"Error: DataFrame '{df_name}' not found. Please extract or create the DataFrame first."
        
        df = g[df_name]
        if not isinstance(df, pd.DataFrame):
            return f"Error: '{df_name}' is not a pandas DataFrame."
        
        # =======================================================================
        # COMPREHENSIVE PREVIEW REPORT GENERATION
        # ç»¼åˆé¢„è§ˆæŠ¥å‘Šç”Ÿæˆ
        # =======================================================================
        
        # Initialize structured report with professional formatting
        # ä½¿ç”¨ä¸“ä¸šæ ¼å¼åˆå§‹åŒ–ç»“æ„åŒ–æŠ¥å‘Š
        preview_report = []
        preview_report.append(f"{'='*60}")
        preview_report.append(f"   DATA PREVIEW SNAPSHOT FOR '{df_name.upper()}'")
        preview_report.append(f"   æ•°æ®é¢„è§ˆå¿«ç…§ - '{df_name.upper()}'")
        preview_report.append(f"{'='*60}")
        preview_report.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | ç”Ÿæˆæ—¶é—´")
        preview_report.append("")
        
        # =======================================================================
        # DATASET STRUCTURE AND PERFORMANCE METRICS
        # æ•°æ®é›†ç»“æ„å’Œæ€§èƒ½æŒ‡æ ‡
        # =======================================================================
        
        # Calculate comprehensive structural metrics
        # è®¡ç®—ç»¼åˆç»“æ„æŒ‡æ ‡
        total_cells = df.shape[0] * df.shape[1]
        memory_mb = df.memory_usage(deep=True).sum() / 1024**2
        memory_per_row = memory_mb / df.shape[0] if df.shape[0] > 0 else 0
        
        preview_report.append("ğŸ“Š DATASET STRUCTURE & PERFORMANCE | æ•°æ®é›†ç»“æ„å’Œæ€§èƒ½:")
        preview_report.append(f"  â€¢ Dimensions: {df.shape[0]:,} rows Ã— {df.shape[1]} columns ({total_cells:,} total cells)")
        preview_report.append(f"  â€¢ Memory Usage: {memory_mb:.2f} MB ({memory_per_row:.3f} MB per row)")
        preview_report.append(f"  â€¢ Index Type: {type(df.index).__name__} (Range: {df.index[0]} to {df.index[-1]} if len(df) > 0 else 'Empty')")
        
        # Performance assessment
        # æ€§èƒ½è¯„ä¼°
        if memory_mb > 100:
            preview_report.append(f"  âš ï¸  Large dataset detected - consider chunked processing for better performance")
        if df.shape[1] > 50:
            preview_report.append(f"  âš ï¸  High dimensionality ({df.shape[1]} columns) - feature selection recommended")
            
        preview_report.append("")
        
        # =======================================================================
        # DATA TYPE ANALYSIS WITH OPTIMIZATION RECOMMENDATIONS
        # æ•°æ®ç±»å‹åˆ†æåŠä¼˜åŒ–å»ºè®®
        # =======================================================================
        
        # Analyze data types and provide optimization insights
        # åˆ†ææ•°æ®ç±»å‹å¹¶æä¾›ä¼˜åŒ–æ´å¯Ÿ
        numeric_cols = df.select_dtypes(include=['number']).columns
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        datetime_cols = df.select_dtypes(include=['datetime']).columns
        
        preview_report.append("ğŸ·ï¸  DATA TYPE ANALYSIS | æ•°æ®ç±»å‹åˆ†æ:")
        preview_report.append(f"  ğŸ”¢ Numeric Columns ({len(numeric_cols)}): {', '.join(numeric_cols[:5])}{'...' if len(numeric_cols) > 5 else ''}")
        preview_report.append(f"  ğŸ“ Categorical Columns ({len(categorical_cols)}): {', '.join(categorical_cols[:5])}{'...' if len(categorical_cols) > 5 else ''}")
        preview_report.append(f"  ğŸ“… DateTime Columns ({len(datetime_cols)}): {', '.join(datetime_cols[:5])}{'...' if len(datetime_cols) > 5 else ''}")
        
        # Detailed type breakdown with optimization suggestions
        # è¯¦ç»†ç±»å‹åˆ†è§£åŠä¼˜åŒ–å»ºè®®
        preview_report.append("  \n  ğŸ” Detailed Type Breakdown:")
        for col, dtype in df.dtypes.items():
            memory_usage = df[col].memory_usage(deep=True) / 1024**2
            if str(dtype) == 'object' and df[col].nunique() < len(df) * 0.5:
                preview_report.append(f"    â€¢ {col}: {dtype} ({memory_usage:.2f}MB) - Consider category type for memory optimization")
            elif 'int64' in str(dtype) and df[col].max() < 2**31:
                preview_report.append(f"    â€¢ {col}: {dtype} ({memory_usage:.2f}MB) - Could use int32 to save memory")
            else:
                preview_report.append(f"    â€¢ {col}: {dtype} ({memory_usage:.2f}MB)")
        preview_report.append("")
        
        # Missing values analysis
        missing = df.isnull().sum()
        missing_pct = (missing / len(df) * 100).round(2)
        preview_report.append("â“ MISSING VALUES:")
        for col in df.columns:
            if missing[col] > 0:
                preview_report.append(f"  â€¢ {col}: {missing[col]} ({missing_pct[col]}%)")
        if missing.sum() == 0:
            preview_report.append("  â€¢ No missing values found âœ“")
        preview_report.append("")
        
        # Numeric columns summary
        numeric_cols = df.select_dtypes(include=['number']).columns
        if len(numeric_cols) > 0:
            preview_report.append("ğŸ“ˆ NUMERIC COLUMNS SUMMARY:")
            desc = df[numeric_cols].describe()
            for col in numeric_cols:
                preview_report.append(f"  â€¢ {col}:")
                preview_report.append(f"    - Range: {desc.loc['min', col]:.2f} to {desc.loc['max', col]:.2f}")
                preview_report.append(f"    - Mean: {desc.loc['mean', col]:.2f}")
                preview_report.append(f"    - Std: {desc.loc['std', col]:.2f}")
            preview_report.append("")
        
        # Categorical columns info
        cat_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(cat_cols) > 0:
            preview_report.append("ğŸ·ï¸  CATEGORICAL COLUMNS:")
            for col in cat_cols:
                unique_count = df[col].nunique()
                preview_report.append(f"  â€¢ {col}: {unique_count} unique values")
                if unique_count <= 10:
                    values = df[col].value_counts().head(5)
                    preview_report.append(f"    - Top values: {list(values.index)}")
            preview_report.append("")
        
        # =======================================================================
        # INTELLIGENT SAMPLE DATA PRESENTATION
        # æ™ºèƒ½æ ·æœ¬æ•°æ®å±•ç¤º
        # =======================================================================
        
        # Generate representative sample with intelligent column selection
        # ç”Ÿæˆå…·æœ‰æ™ºèƒ½åˆ—é€‰æ‹©çš„ä»£è¡¨æ€§æ ·æœ¬
        sample_size = min(rows, len(df))
        
        preview_report.append(f"ğŸ“‹ SAMPLE DATA PREVIEW | æ ·æœ¬æ•°æ®é¢„è§ˆ (First {sample_size} rows):")
        preview_report.append(f"   Showing {sample_size} of {len(df):,} total rows ({sample_size/len(df)*100:.1f}% sample)")
        preview_report.append("")
        
        # Optimize column display for readability
        # ä¼˜åŒ–åˆ—æ˜¾ç¤ºä»¥æé«˜å¯è¯»æ€§
        if df.shape[1] > 10:
            # For wide DataFrames, show most important columns
            # å¯¹äºå®½ DataFrameï¼Œæ˜¾ç¤ºæœ€é‡è¦çš„åˆ—
            numeric_sample = df[numeric_cols[:3]] if len(numeric_cols) > 0 else pd.DataFrame()
            categorical_sample = df[categorical_cols[:3]] if len(categorical_cols) > 0 else pd.DataFrame()
            other_cols = [col for col in df.columns if col not in list(numeric_cols[:3]) + list(categorical_cols[:3])][:4]
            
            sample_df = pd.concat([numeric_sample, categorical_sample, df[other_cols]], axis=1)
            preview_report.append(f"   Note: Showing {len(sample_df.columns)} most representative columns of {df.shape[1]} total")
            sample_data = sample_df.head(sample_size).to_string(max_cols=15, max_colwidth=25)
        else:
            sample_data = df.head(sample_size).to_string(max_cols=15, max_colwidth=25)
            
        preview_report.append(sample_data)
        
        # Add data preview summary
        # æ·»åŠ æ•°æ®é¢„è§ˆæ‘˜è¦
        preview_report.append("")
        preview_report.append("ğŸ“Š PREVIEW SUMMARY | é¢„è§ˆæ‘˜è¦:")
        preview_report.append(f"  âœ… Dataset loaded successfully with {df.shape[0]:,} records")
        preview_report.append(f"  ğŸ“ˆ Ready for analysis - use python_inter or other tools for deeper insights")
        preview_report.append(f"  ğŸ“ Export options available in Excel, JSON, and PDF formats")
        
        return "\n".join(preview_report)
        
    except KeyError:
        # Handle case where DataFrame variable doesn't exist
        # å¤„ç† DataFrame å˜é‡ä¸å­˜åœ¨çš„æƒ…å†µ
        return f"Error: DataFrame '{df_name}' not found in global scope. Use extract_data tool first to load data."
        
    except TypeError as type_error:
        # Handle case where variable exists but isn't a DataFrame
        # å¤„ç†å˜é‡å­˜åœ¨ä½†ä¸æ˜¯ DataFrame çš„æƒ…å†µ
        return f"Error: Variable '{df_name}' exists but is not a pandas DataFrame: {str(type_error)}"
        
    except MemoryError:
        # Handle memory issues with large datasets
        # å¤„ç†å¤§å‹æ•°æ®é›†çš„å†…å­˜é—®é¢˜
        return f"Memory error: Dataset '{df_name}' too large for preview. Try reducing sample size or use chunked analysis."
        
    except Exception as e:
        # Handle any other unexpected errors
        # å¤„ç†ä»»ä½•å…¶ä»–æ„å¤–é”™è¯¯
        return f"Preview generation failed for '{df_name}': {str(e)}"

# ============================================================================
# QUICK CHART TEMPLATE TOOL CONFIGURATION
# å¿«é€Ÿå›¾è¡¨æ¨¡æ¿å·¥å…·é…ç½®
# ============================================================================
# Professional chart generation with predefined templates for rapid visualization
# ä½¿ç”¨é¢„å®šä¹‰æ¨¡æ¿è¿›è¡Œä¸“ä¸šå›¾è¡¨ç”Ÿæˆï¼Œå®ç°å¿«é€Ÿå¯è§†åŒ–
# ============================================================================
class ChartTemplateSchema(BaseModel):
    df_name: str = Field(description="Name of the pandas DataFrame variable for plotting")
    chart_type: str = Field(description="Chart type: 'scatter', 'bar', 'heatmap'")
    x_col: str = Field(description="Column name for X-axis")
    y_col: str = Field(description="Column name for Y-axis")
    title: str = Field(default="Chart", description="Chart title")

@tool(args_schema=ChartTemplateSchema)
def quick_chart(df_name: str, chart_type: str, x_col: str, y_col: str, title: str = "Chart") -> str:
    """
    Generate quick charts using predefined templates (scatter plot, bar chart, heatmap).
    Provides instant visualization with professional styling and automatic layout.
    
    :param df_name: Name of the pandas DataFrame variable for plotting
    :param chart_type: Type of chart - 'scatter', 'bar', or 'heatmap'
    :param x_col: Column name for X-axis (not used for heatmap)
    :param y_col: Column name for Y-axis (not used for heatmap)
    :param title: Chart title
    :return: Chart generation status and file path
    """
    try:
        # Get DataFrame from global variables
        g = globals()
        if df_name not in g:
            return f"Error: DataFrame '{df_name}' not found. Please extract or create the DataFrame first."
        
        df = g[df_name]
        if not isinstance(df, pd.DataFrame):
            return f"Error: '{df_name}' is not a pandas DataFrame."
        
        # Set up matplotlib for file saving
        current_backend = matplotlib.get_backend()
        matplotlib.use('Agg')
        
        # Create figure
        try:
            plt.style.use('seaborn-v0_8')
        except:
            # Fallback to default style if seaborn style fails
            plt.style.use('default')
        fig = plt.figure(figsize=(10, 6))
        
        if chart_type.lower() == 'scatter':
            if x_col not in df.columns or y_col not in df.columns:
                return f"Error: Columns '{x_col}' or '{y_col}' not found in DataFrame"
            
            plt.scatter(df[x_col], df[y_col], alpha=0.6, s=50)
            plt.xlabel(x_col)
            plt.ylabel(y_col)
            plt.title(f"{title} - Scatter Plot")
            plt.grid(True, alpha=0.3)
            
        elif chart_type.lower() == 'bar':
            if x_col not in df.columns or y_col not in df.columns:
                return f"Error: Columns '{x_col}' or '{y_col}' not found in DataFrame"
            
            # For bar charts, group by x_col and aggregate y_col
            data = df.groupby(x_col)[y_col].mean().sort_values(ascending=False)
            bars = plt.bar(range(len(data)), data.values)
            plt.xlabel(x_col)
            plt.ylabel(f"Average {y_col}")
            plt.title(f"{title} - Bar Chart")
            plt.xticks(range(len(data)), data.index, rotation=45, ha='right')
            
            # Color bars with gradient
            colors = plt.cm.viridis([i/len(data) for i in range(len(data))])
            for bar, color in zip(bars, colors):
                bar.set_color(color)
            
        elif chart_type.lower() == 'heatmap':
            # For heatmap, use correlation matrix of numeric columns
            numeric_df = df.select_dtypes(include=['number'])
            if numeric_df.empty:
                return "Error: No numeric columns found for heatmap"
            
            corr = numeric_df.corr()
            sns.heatmap(corr, annot=True, cmap='coolwarm', center=0,
                       square=True, fmt='.2f', cbar_kws={'shrink': 0.8})
            plt.title(f"{title} - Correlation Heatmap")
            plt.tight_layout()
            
        else:
            return f"Error: Unsupported chart type '{chart_type}'. Supported types: scatter, bar, heatmap"
        
        # Save figure
        base_dir = os.getenv('PUBLIC_DIR', "/app/shared/public")
        images_dir = os.path.join(base_dir, "images")
        os.makedirs(images_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"quick_{chart_type}_{timestamp}.png"
        abs_path = os.path.join(images_dir, filename)
        rel_path = os.path.join("images", filename)
        
        fig.tight_layout()
        fig.savefig(abs_path, bbox_inches='tight', dpi=300)
        
        return f"Chart saved successfully: {rel_path}\n\n---\n\n![{title} - {chart_type.title()} Chart]({rel_path})"
        
    except Exception as e:
        return f"Chart generation failed: {str(e)}"
    finally:
        plt.close('all')
        matplotlib.use(current_backend)

# Create SQL query history tool / åˆ›å»ºSQLæŸ¥è¯¢å†å²å·¥å…·
class QueryHistorySchema(BaseModel):
    action: str = Field(description="Action: 'save', 'list', 'reuse'")
    query: str = Field(default="", description="SQL query to save (for 'save' action)")
    query_id: int = Field(default=0, description="Query ID to reuse (for 'reuse' action)")
    description: str = Field(default="", description="Description of the query (for 'save' action)")

@tool(args_schema=QueryHistorySchema)
def query_history(action: str, query: str = "", query_id: int = 0, description: str = "") -> str:
    """
    Manage SQL query history for quick reuse and reference.
    Save frequently used queries, list query history, and reuse previous queries.
    
    :param action: Action to perform - 'save', 'list', or 'reuse'
    :param query: SQL query string (for 'save' action)
    :param query_id: ID of query to reuse (for 'reuse' action)
    :param description: Optional description for the query (for 'save' action)
    :return: Operation result
    """
    try:
        # History file path
        base_dir = os.getenv('PROJECT_ROOT', "/app")
        history_file = os.path.join(base_dir, "query_history.json")
        
        # Load existing history
        if os.path.exists(history_file):
            with open(history_file, 'r', encoding='utf-8') as f:
                history = json.load(f)
        else:
            history = {"queries": [], "next_id": 1}
        
        if action.lower() == 'save':
            if not query.strip():
                return "Error: Query cannot be empty for save action"
            
            # Add new query to history
            new_entry = {
                "id": history["next_id"],
                "query": query.strip(),
                "description": description.strip(),
                "timestamp": datetime.now().isoformat(),
                "usage_count": 0
            }
            
            history["queries"].append(new_entry)
            history["next_id"] += 1
            
            # Save updated history
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(history, f, indent=2, ensure_ascii=False)
            
            return f"Query saved successfully with ID: {new_entry['id']}"
            
        elif action.lower() == 'list':
            if not history["queries"]:
                return "No queries in history"
            
            result = ["=== SQL QUERY HISTORY ==="]
            for entry in sorted(history["queries"], key=lambda x: x["timestamp"], reverse=True):
                result.append(f"\nID: {entry['id']}")
                result.append(f"Description: {entry['description'] or 'No description'}")
                result.append(f"Query: {entry['query'][:100]}{'...' if len(entry['query']) > 100 else ''}")
                result.append(f"Used: {entry['usage_count']} times")
                result.append(f"Date: {entry['timestamp'][:19].replace('T', ' ')}")
            
            return "\n".join(result)
            
        elif action.lower() == 'reuse':
            if query_id <= 0:
                return "Error: Please provide a valid query ID for reuse action"
            
            # Find query by ID
            target_query = None
            for entry in history["queries"]:
                if entry["id"] == query_id:
                    target_query = entry
                    break
            
            if not target_query:
                return f"Error: Query with ID {query_id} not found"
            
            # Update usage count
            target_query["usage_count"] += 1
            target_query["last_used"] = datetime.now().isoformat()
            
            # Save updated history
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(history, f, indent=2, ensure_ascii=False)
            
            return f"Reusing query ID {query_id}:\n{target_query['query']}"
            
        else:
            return f"Error: Invalid action '{action}'. Supported actions: save, list, reuse"
            
    except Exception as e:
        return f"Query history operation failed: {str(e)}"

# ============================================================================
# COMPREHENSIVE DATA QUALITY ASSESSMENT TOOL
# ç»¼åˆæ•°æ®è´¨é‡è¯„ä¼°å·¥å…·
# ============================================================================
# This tool provides enterprise-grade data quality assessment capabilities
# è¯¥å·¥å…·æä¾›ä¼ä¸šçº§æ•°æ®è´¨é‡è¯„ä¼°åŠŸèƒ½
# Key features: Missing value analysis, duplicate detection, outlier identification, data type validation
# ä¸»è¦åŠŸèƒ½ï¼šç¼ºå¤±å€¼åˆ†æã€é‡å¤å€¼æ£€æµ‹ã€å¼‚å¸¸å€¼è¯†åˆ«ã€æ•°æ®ç±»å‹éªŒè¯

# Data quality assessment schema / æ•°æ®è´¨é‡è¯„ä¼°æ¨¡å¼
class DataQualitySchema(BaseModel):
    """
    Input schema for comprehensive data quality assessment tool
    ç»¼åˆæ•°æ®è´¨é‡è¯„ä¼°å·¥å…·çš„è¾“å…¥æ¨¡å¼
    
    This schema validates user input for data quality checks ensuring proper parameter types
    è¯¥æ¨¡å¼éªŒè¯æ•°æ®è´¨é‡æ£€æŸ¥çš„ç”¨æˆ·è¾“å…¥ï¼Œç¡®ä¿å‚æ•°ç±»å‹æ­£ç¡®
    """
    df_name: str = Field(description="Name of the pandas DataFrame variable to check / è¦æ£€æŸ¥çš„pandas DataFrameå˜é‡å")
    check_types: str = Field(default="all", description="Types of checks: 'all', 'missing', 'duplicates', 'outliers', 'types' / æ£€æŸ¥ç±»å‹ï¼š'all'(å…¨éƒ¨), 'missing'(ç¼ºå¤±å€¼), 'duplicates'(é‡å¤å€¼), 'outliers'(å¼‚å¸¸å€¼), 'types'(æ•°æ®ç±»å‹)")

@tool(args_schema=DataQualitySchema)
def data_quality_check(df_name: str, check_types: str = "all") -> str:
    """
    COMPREHENSIVE DATA QUALITY ASSESSMENT FUNCTION
    ç»¼åˆæ•°æ®è´¨é‡è¯„ä¼°åŠŸèƒ½
    
    This function performs thorough data quality analysis to identify potential issues in datasets.
    è¯¥å‡½æ•°æ‰§è¡Œå…¨é¢çš„æ•°æ®è´¨é‡åˆ†æï¼Œè¯†åˆ«æ•°æ®é›†ä¸­çš„æ½œåœ¨é—®é¢˜ã€‚
    
    BUSINESS VALUE / ä¸šåŠ¡ä»·å€¼:
    - Early detection of data issues before analysis / åœ¨åˆ†æå‰åŠæ—©å‘ç°æ•°æ®é—®é¢˜
    - Automated quality reporting for stakeholders / ä¸ºåˆ©ç›Šç›¸å…³è€…æä¾›è‡ªåŠ¨åŒ–è´¨é‡æŠ¥å‘Š
    - Standardized quality metrics across projects / è·¨é¡¹ç›®çš„æ ‡å‡†åŒ–è´¨é‡æŒ‡æ ‡
    - Risk mitigation for data-driven decisions / é™ä½æ•°æ®é©±åŠ¨å†³ç­–çš„é£é™©
    
    TECHNICAL CAPABILITIES / æŠ€æœ¯èƒ½åŠ›:
    1. Missing Values Analysis: Percentage and distribution / ç¼ºå¤±å€¼åˆ†æï¼šç™¾åˆ†æ¯”å’Œåˆ†å¸ƒ
    2. Duplicate Detection: Full record duplicates / é‡å¤æ£€æµ‹ï¼šå®Œæ•´è®°å½•é‡å¤
    3. Outlier Identification: IQR-based statistical outliers / å¼‚å¸¸å€¼è¯†åˆ«ï¼šåŸºäºIQRçš„ç»Ÿè®¡å¼‚å¸¸å€¼
    4. Data Type Validation: Type consistency and format issues / æ•°æ®ç±»å‹éªŒè¯ï¼šç±»å‹ä¸€è‡´æ€§å’Œæ ¼å¼é—®é¢˜
    5. Quality Scoring: Overall quality assessment / è´¨é‡è¯„åˆ†ï¼šæ•´ä½“è´¨é‡è¯„ä¼°
    
    WORKFLOW PROCESS / å·¥ä½œæµç¨‹:
    Step 1: Validate DataFrame existence and type / æ­¥éª¤1ï¼šéªŒè¯DataFrameå­˜åœ¨æ€§å’Œç±»å‹
    Step 2: Initialize quality report structure / æ­¥éª¤2ï¼šåˆå§‹åŒ–è´¨é‡æŠ¥å‘Šç»“æ„
    Step 3: Execute selected quality checks / æ­¥éª¤3ï¼šæ‰§è¡Œé€‰å®šçš„è´¨é‡æ£€æŸ¥
    Step 4: Aggregate findings and calculate scores / æ­¥éª¤4ï¼šæ±‡æ€»å‘ç°å¹¶è®¡ç®—è¯„åˆ†
    Step 5: Generate actionable recommendations / æ­¥éª¤5ï¼šç”Ÿæˆå¯æ“ä½œçš„å»ºè®®
    Step 6: Return comprehensive quality report / æ­¥éª¤6ï¼šè¿”å›ç»¼åˆè´¨é‡æŠ¥å‘Š
    
    :param df_name: Name of the pandas DataFrame variable to check / è¦æ£€æŸ¥çš„pandas DataFrameå˜é‡å
    :param check_types: Types of checks to perform - 'all', 'missing', 'duplicates', 'outliers', 'types' / è¦æ‰§è¡Œçš„æ£€æŸ¥ç±»å‹
    :return: Comprehensive data quality report with severity indicators and recommendations / åŒ…å«ä¸¥é‡æ€§æŒ‡æ ‡å’Œå»ºè®®çš„ç»¼åˆæ•°æ®è´¨é‡æŠ¥å‘Š
    """
    try:
        # ========================================================================
        # STEP 1: DATAFRAME VALIDATION AND INITIALIZATION
        # æ­¥éª¤1ï¼šDataFrameéªŒè¯å’Œåˆå§‹åŒ–
        # ========================================================================
        
        # Retrieve DataFrame from global namespace (where extract_data saves it)
        # ä»å…¨å±€å‘½åç©ºé—´è·å–DataFrameï¼ˆextract_dataä¿å­˜çš„ä½ç½®ï¼‰
        g = globals()
        if df_name not in g:
            return f"Error: DataFrame '{df_name}' not found. Please extract or create the DataFrame first."
        
        # Ensure the retrieved object is actually a pandas DataFrame
        # ç¡®ä¿è·å–çš„å¯¹è±¡ç¡®å®æ˜¯pandas DataFrame
        df = g[df_name]
        if not isinstance(df, pd.DataFrame):
            return f"Error: '{df_name}' is not a pandas DataFrame."
        
        # ========================================================================
        # STEP 2: INITIALIZE COMPREHENSIVE QUALITY REPORT STRUCTURE
        # æ­¥éª¤2ï¼šåˆå§‹åŒ–ç»¼åˆè´¨é‡æŠ¥å‘Šç»“æ„
        # ========================================================================
        
        # Create structured report with headers and metadata
        # åˆ›å»ºå¸¦æœ‰æ ‡é¢˜å’Œå…ƒæ•°æ®çš„ç»“æ„åŒ–æŠ¥å‘Š
        report = []
        report.append(f"=== DATA QUALITY REPORT FOR '{df_name}' ===")
        report.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"Dataset: {df.shape[0]} rows Ã— {df.shape[1]} columns")
        report.append("")
        
        # Initialize issue counter for overall quality scoring
        # åˆå§‹åŒ–é—®é¢˜è®¡æ•°å™¨ï¼Œç”¨äºæ•´ä½“è´¨é‡è¯„åˆ†
        issues_found = 0
        
        # ========================================================================
        # STEP 3A: MISSING VALUES ANALYSIS
        # æ­¥éª¤3Aï¼šç¼ºå¤±å€¼åˆ†æ
        # ========================================================================
        
        # Perform comprehensive missing value detection and analysis
        # æ‰§è¡Œç»¼åˆç¼ºå¤±å€¼æ£€æµ‹å’Œåˆ†æ
        if check_types.lower() in ['all', 'missing']:
            report.append("ğŸ” MISSING VALUES ANALYSIS:")
            
            # Calculate missing values count and percentage for each column
            # è®¡ç®—æ¯åˆ—çš„ç¼ºå¤±å€¼æ•°é‡å’Œç™¾åˆ†æ¯”
            missing = df.isnull().sum()  # Count of missing values per column / æ¯åˆ—ç¼ºå¤±å€¼æ•°é‡
            missing_pct = (missing / len(df) * 100).round(2)  # Percentage of missing values / ç¼ºå¤±å€¼ç™¾åˆ†æ¯”
            
            # Evaluate overall missing data situation
            # è¯„ä¼°æ•´ä½“ç¼ºå¤±æ•°æ®æƒ…å†µ
            if missing.sum() == 0:
                report.append("  âœ… No missing values found")
            else:
                # Add to global issues counter for quality scoring
                # æ·»åŠ åˆ°å…¨å±€é—®é¢˜è®¡æ•°å™¨ç”¨äºè´¨é‡è¯„åˆ†
                issues_found += missing.sum()
                report.append(f"  âš ï¸  Total missing values: {missing.sum()}")
                
                # Analyze each column with missing values and assign severity levels
                # åˆ†ææ¯ä¸ªæœ‰ç¼ºå¤±å€¼çš„åˆ—å¹¶åˆ†é…ä¸¥é‡æ€§çº§åˆ«
                for col in df.columns:
                    if missing[col] > 0:
                        # Severity classification based on missing percentage
                        # åŸºäºç¼ºå¤±ç™¾åˆ†æ¯”çš„ä¸¥é‡æ€§åˆ†ç±»
                        # Red: >50% missing (critical), Yellow: >10% missing (warning), Green: <10% missing (minor)
                        # çº¢è‰²ï¼š>50%ç¼ºå¤±ï¼ˆä¸¥é‡ï¼‰ï¼Œé»„è‰²ï¼š>10%ç¼ºå¤±ï¼ˆè­¦å‘Šï¼‰ï¼Œç»¿è‰²ï¼š<10%ç¼ºå¤±ï¼ˆè½»å¾®ï¼‰
                        severity = "ğŸ”´" if missing_pct[col] > 50 else "ğŸŸ¡" if missing_pct[col] > 10 else "ğŸŸ¢"
                        report.append(f"    {severity} {col}: {missing[col]} ({missing_pct[col]}%)")
            report.append("")
        
        # ========================================================================
        # STEP 3B: DUPLICATE RECORDS DETECTION AND ANALYSIS
        # æ­¥éª¤3Bï¼šé‡å¤è®°å½•æ£€æµ‹å’Œåˆ†æ
        # ========================================================================
        
        # Identify complete duplicate rows that may skew analysis results
        # è¯†åˆ«å¯èƒ½æ­ªæ›²åˆ†æç»“æœçš„å®Œæ•´é‡å¤è¡Œ
        if check_types.lower() in ['all', 'duplicates']:
            report.append("ğŸ” DUPLICATE RECORDS ANALYSIS:")
            
            # Count total duplicate rows using pandas duplicated() method
            # ä½¿ç”¨pandas duplicated()æ–¹æ³•è®¡ç®—æ€»é‡å¤è¡Œæ•°
            # This identifies rows that are exact duplicates of previous rows
            # è¿™è¯†åˆ«ä¸ä¹‹å‰è¡Œå®Œå…¨é‡å¤çš„è¡Œ
            duplicates = df.duplicated().sum()
            
            # Evaluate duplicate data situation
            # è¯„ä¼°é‡å¤æ•°æ®æƒ…å†µ
            if duplicates == 0:
                report.append("  âœ… No duplicate rows found")
            else:
                # Add duplicates to issues counter for overall quality assessment
                # å°†é‡å¤å€¼æ·»åŠ åˆ°é—®é¢˜è®¡æ•°å™¨ç”¨äºæ•´ä½“è´¨é‡è¯„ä¼°
                issues_found += duplicates
                
                # Calculate percentage of duplicate records
                # è®¡ç®—é‡å¤è®°å½•çš„ç™¾åˆ†æ¯”
                duplicate_pct = (duplicates / len(df) * 100).round(2)
                
                # Assign severity level based on duplicate percentage
                # æ ¹æ®é‡å¤ç™¾åˆ†æ¯”åˆ†é…ä¸¥é‡æ€§çº§åˆ«
                # Red: >10% duplicates (critical data integrity issue)
                # Yellow: >5% duplicates (moderate concern)
                # Green: <5% duplicates (minor issue)
                # çº¢è‰²ï¼š>10%é‡å¤ï¼ˆä¸¥é‡æ•°æ®å®Œæ•´æ€§é—®é¢˜ï¼‰
                # é»„è‰²ï¼š>5%é‡å¤ï¼ˆä¸­ç­‰å…³æ³¨ï¼‰
                # ç»¿è‰²ï¼š<5%é‡å¤ï¼ˆè½»å¾®é—®é¢˜ï¼‰
                severity = "ğŸ”´" if duplicate_pct > 10 else "ğŸŸ¡" if duplicate_pct > 5 else "ğŸŸ¢"
                report.append(f"  {severity} Duplicate rows: {duplicates} ({duplicate_pct}%)")
            report.append("")
        
        # ========================================================================
        # STEP 3C: DATA TYPE CONSISTENCY AND FORMAT VALIDATION
        # æ­¥éª¤3Cï¼šæ•°æ®ç±»å‹ä¸€è‡´æ€§å’Œæ ¼å¼éªŒè¯
        # ========================================================================
        
        # Analyze data type appropriateness and detect format inconsistencies
        # åˆ†ææ•°æ®ç±»å‹é€‚å½“æ€§å¹¶æ£€æµ‹æ ¼å¼ä¸ä¸€è‡´æ€§
        if check_types.lower() in ['all', 'types']:
            report.append("ğŸ” DATA TYPE ANALYSIS:")
            type_issues = 0  # Counter for data type related issues / æ•°æ®ç±»å‹ç›¸å…³é—®é¢˜è®¡æ•°å™¨
            
            # Iterate through each column to check data type appropriateness
            # éå†æ¯åˆ—æ£€æŸ¥æ•°æ®ç±»å‹é€‚å½“æ€§
            for col in df.columns:
                col_type = df[col].dtype
                
                # Focus on 'object' type columns which may have hidden issues
                # ä¸“æ³¨äº'object'ç±»å‹åˆ—ï¼Œå¯èƒ½å­˜åœ¨éšè—é—®é¢˜
                if col_type == 'object':
                    
                    # ================================================================
                    # SUB-CHECK 1: NUMERIC DATA STORED AS TEXT
                    # å­æ£€æŸ¥1ï¼šä»¥æ–‡æœ¬å½¢å¼å­˜å‚¨çš„æ•°å€¼æ•°æ®
                    # ================================================================
                    # This can cause performance issues and prevent proper statistical analysis
                    # è¿™å¯èƒ½å¯¼è‡´æ€§èƒ½é—®é¢˜å¹¶é˜»æ­¢æ­£å¸¸çš„ç»Ÿè®¡åˆ†æ
                    try:
                        # Attempt to convert to numeric, 'coerce' makes non-numeric values NaN
                        # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ï¼Œ'coerce'ä½¿éæ•°å€¼å˜ä¸ºNaN
                        numeric_conversion = pd.to_numeric(df[col], errors='coerce')
                        
                        # Count how many values couldn't be converted (excluding original NaNs)
                        # è®¡ç®—æ— æ³•è½¬æ¢çš„å€¼æ•°é‡ï¼ˆæ’é™¤åŸå§‹NaNï¼‰
                        non_numeric = numeric_conversion.isnull().sum() - df[col].isnull().sum()
                        
                        # If all non-null values can be converted to numeric, flag as issue
                        # å¦‚æœæ‰€æœ‰éç©ºå€¼éƒ½å¯ä»¥è½¬æ¢ä¸ºæ•°å€¼ï¼Œæ ‡è®°ä¸ºé—®é¢˜
                        if non_numeric == 0 and not df[col].isnull().all():
                            type_issues += 1
                            report.append(f"    ğŸŸ¡ {col}: Numeric data stored as text")
                    except:
                        pass  # Skip columns that can't be analyzed / è·³è¿‡æ— æ³•åˆ†æçš„åˆ—
                    
                    # ================================================================
                    # SUB-CHECK 2: INCONSISTENT CASE IN CATEGORICAL DATA
                    # å­æ£€æŸ¥2ï¼šåˆ†ç±»æ•°æ®ä¸­çš„ä¸ä¸€è‡´å¤§å°å†™
                    # ================================================================
                    # Mixed case can create artificial categories and skew analysis
                    # æ··åˆå¤§å°å†™å¯èƒ½åˆ›å»ºäººå·¥åˆ†ç±»å¹¶æ­ªæ›²åˆ†æ
                    if df[col].nunique() < len(df) * 0.5:  # Likely categorical if unique values < 50% of total rows
                        # å¦‚æœå”¯ä¸€å€¼ < æ€»è¡Œæ•°50%ï¼Œåˆ™å¯èƒ½æ˜¯åˆ†ç±»æ•°æ®
                        unique_vals = df[col].dropna().astype(str)
                        
                        # Check if lowercasing reduces the number of unique values
                        # æ£€æŸ¥è½¬æ¢ä¸ºå°å†™æ˜¯å¦å‡å°‘äº†å”¯ä¸€å€¼çš„æ•°é‡
                        case_variants = unique_vals.str.lower().nunique() < unique_vals.nunique()
                        if case_variants:
                            type_issues += 1
                            report.append(f"    ğŸŸ¡ {col}: Inconsistent case in categorical data")
            
            # Summarize data type analysis results
            # æ€»ç»“æ•°æ®ç±»å‹åˆ†æç»“æœ
            if type_issues == 0:
                report.append("  âœ… No data type issues found")
            else:
                issues_found += type_issues
                report.append(f"  âš ï¸  Data type issues found: {type_issues}")
            report.append("")
        
        # ========================================================================
        # STEP 3D: STATISTICAL OUTLIER DETECTION AND ANALYSIS
        # æ­¥éª¤3Dï¼šç»Ÿè®¡å¼‚å¸¸å€¼æ£€æµ‹å’Œåˆ†æ
        # ========================================================================
        
        # Identify statistical outliers using Interquartile Range (IQR) method
        # ä½¿ç”¨å››åˆ†ä½æ•°èŒƒå›´(IQR)æ–¹æ³•è¯†åˆ«ç»Ÿè®¡å¼‚å¸¸å€¼
        if check_types.lower() in ['all', 'outliers']:
            report.append("ğŸ” OUTLIERS ANALYSIS:")
            
            # Select only numeric columns for outlier analysis
            # ä»…é€‰æ‹©æ•°å€¼åˆ—è¿›è¡Œå¼‚å¸¸å€¼åˆ†æ
            numeric_cols = df.select_dtypes(include=['number']).columns
            
            # Check if any numeric columns exist for analysis
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¯åˆ†æçš„æ•°å€¼åˆ—
            if len(numeric_cols) == 0:
                report.append("  â„¹ï¸  No numeric columns to check for outliers")
            else:
                outlier_cols = 0  # Counter for columns with outliers / æœ‰å¼‚å¸¸å€¼çš„åˆ—è®¡æ•°å™¨
                
                # Analyze each numeric column for statistical outliers
                # åˆ†ææ¯ä¸ªæ•°å€¼åˆ—çš„ç»Ÿè®¡å¼‚å¸¸å€¼
                for col in numeric_cols:
                    # ============================================================
                    # IQR METHOD FOR OUTLIER DETECTION
                    # IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
                    # ============================================================
                    # This method defines outliers as values outside Q1-1.5*IQR to Q3+1.5*IQR
                    # è¯¥æ–¹æ³•å°†å¼‚å¸¸å€¼å®šä¹‰ä¸ºåœ¨Q1-1.5*IQRåˆ°Q3+1.5*IQRèŒƒå›´å¤–çš„å€¼
                    
                    # Calculate quartiles for the column
                    # è®¡ç®—åˆ—çš„å››åˆ†ä½æ•°
                    Q1 = df[col].quantile(0.25)  # First quartile (25th percentile) / ç¬¬ä¸€å››åˆ†ä½æ•°ï¼ˆ25%ç™¾åˆ†ä½æ•°ï¼‰
                    Q3 = df[col].quantile(0.75)  # Third quartile (75th percentile) / ç¬¬ä¸‰å››åˆ†ä½æ•°ï¼ˆ75%ç™¾åˆ†ä½æ•°ï¼‰
                    IQR = Q3 - Q1  # Interquartile Range / å››åˆ†ä½æ•°èŒƒå›´
                    
                    # Define outlier boundaries using 1.5 * IQR rule
                    # ä½¿ç”¨1.5 * IQRè§„åˆ™å®šä¹‰å¼‚å¸¸å€¼è¾¹ç•Œ
                    lower_bound = Q1 - 1.5 * IQR  # Values below this are outliers / ä½äºæ­¤å€¼çš„ä¸ºå¼‚å¸¸å€¼
                    upper_bound = Q3 + 1.5 * IQR  # Values above this are outliers / é«˜äºæ­¤å€¼çš„ä¸ºå¼‚å¸¸å€¼
                    
                    # Count outliers in the column
                    # è®¡ç®—åˆ—ä¸­çš„å¼‚å¸¸å€¼
                    outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
                    
                    # Report outliers if found
                    # å¦‚æœå‘ç°å¼‚å¸¸å€¼åˆ™æŠ¥å‘Š
                    if outliers > 0:
                        outlier_cols += 1
                        
                        # Calculate percentage of outliers
                        # è®¡ç®—å¼‚å¸¸å€¼ç™¾åˆ†æ¯”
                        outlier_pct = (outliers / len(df) * 100).round(2)
                        
                        # Assign severity based on outlier percentage
                        # æ ¹æ®å¼‚å¸¸å€¼ç™¾åˆ†æ¯”åˆ†é…ä¸¥é‡æ€§
                        # Red: >10% outliers (potential data quality issue)
                        # Yellow: >5% outliers (worth investigating)
                        # Green: <5% outliers (normal statistical variation)
                        # çº¢è‰²ï¼š>10%å¼‚å¸¸å€¼ï¼ˆæ½œåœ¨æ•°æ®è´¨é‡é—®é¢˜ï¼‰
                        # é»„è‰²ï¼š>5%å¼‚å¸¸å€¼ï¼ˆå€¼å¾—è°ƒæŸ¥ï¼‰
                        # ç»¿è‰²ï¼š<5%å¼‚å¸¸å€¼ï¼ˆæ­£å¸¸ç»Ÿè®¡å˜åŒ–ï¼‰
                        severity = "ğŸ”´" if outlier_pct > 10 else "ğŸŸ¡" if outlier_pct > 5 else "ğŸŸ¢"
                        report.append(f"    {severity} {col}: {outliers} outliers ({outlier_pct}%)")
                
                # Summarize outlier analysis results
                # æ€»ç»“å¼‚å¸¸å€¼åˆ†æç»“æœ
                if outlier_cols == 0:
                    report.append("  âœ… No significant outliers found")
                else:
                    issues_found += outlier_cols
            report.append("")
        
        # ========================================================================
        # STEP 4: COMPREHENSIVE QUALITY SUMMARY AND STRATEGIC RECOMMENDATIONS
        # æ­¥éª¤4ï¼šç»¼åˆè´¨é‡æ€»ç»“å’Œæˆ˜ç•¥å»ºè®®
        # ========================================================================
        
        # Generate executive summary with overall quality assessment
        # ç”ŸæˆåŒ…å«æ•´ä½“è´¨é‡è¯„ä¼°çš„æ‰§è¡Œæ€»ç»“
        report.append("ğŸ“Š QUALITY SUMMARY:")
        # Evaluate overall data quality based on total issues found
        # æ ¹æ®å‘ç°çš„æ€»é—®é¢˜æ•°è¯„ä¼°æ•´ä½“æ•°æ®è´¨é‡
        if issues_found == 0:
            # Perfect quality scenario - rare but excellent for analysis
            # å®Œç¾è´¨é‡åœºæ™¯ - ç½•è§ä½†å¯¹åˆ†ææä½³
            report.append("  ğŸ‰ Excellent! No major data quality issues detected")
        else:
            # Quality scoring system based on issue severity and count
            # åŸºäºé—®é¢˜ä¸¥é‡æ€§å’Œæ•°é‡çš„è´¨é‡è¯„åˆ†ç³»ç»Ÿ
            # Poor: >20 issues (requires significant cleanup before analysis)
            # Fair: >10 issues (moderate issues, proceed with caution)
            # Good: <10 issues (minor issues, analysis can proceed)
            # å·®ï¼š>20ä¸ªé—®é¢˜ï¼ˆåˆ†æå‰éœ€è¦å¤§é‡æ¸…ç†ï¼‰
            # ä¸€èˆ¬ï¼š>10ä¸ªé—®é¢˜ï¼ˆä¸­ç­‰é—®é¢˜ï¼Œè°¨æ…è¿›è¡Œï¼‰
            # å¥½ï¼š<10ä¸ªé—®é¢˜ï¼ˆè½»å¾®é—®é¢˜ï¼Œå¯ä»¥è¿›è¡Œåˆ†æï¼‰
            severity = "ğŸ”´ Poor" if issues_found > 20 else "ğŸŸ¡ Fair" if issues_found > 10 else "ğŸŸ¢ Good"
            report.append(f"  Data Quality: {severity}")
            report.append(f"  Total issues detected: {issues_found}")
            
            # ================================================================
            # ACTIONABLE RECOMMENDATIONS BASED ON DETECTED ISSUES
            # åŸºäºæ£€æµ‹é—®é¢˜çš„å¯æ“ä½œå»ºè®®
            # ================================================================
            # Provide specific, prioritized recommendations for data improvement
            # ä¸ºæ•°æ®æ”¹è¿›æä¾›å…·ä½“çš„ã€ä¼˜å…ˆåŒ–çš„å»ºè®®
            report.append("\nğŸ’¡ RECOMMENDATIONS:")
            
            # Missing values recommendation / ç¼ºå¤±å€¼å»ºè®®
            if check_types.lower() in ['all', 'missing'] and df.isnull().sum().sum() > 0:
                report.append("  â€¢ Handle missing values using imputation or removal")
            
            # Duplicate records recommendation / é‡å¤è®°å½•å»ºè®®
            if check_types.lower() in ['all', 'duplicates'] and df.duplicated().sum() > 0:
                report.append("  â€¢ Remove or investigate duplicate records")
            
            # Data type optimization recommendation / æ•°æ®ç±»å‹ä¼˜åŒ–å»ºè®®
            if check_types.lower() in ['all', 'types']:
                report.append("  â€¢ Convert data types for better performance and accuracy")
            
            # Outlier investigation recommendation / å¼‚å¸¸å€¼è°ƒæŸ¥å»ºè®®
            if check_types.lower() in ['all', 'outliers'] and len(df.select_dtypes(include=['number']).columns) > 0:
                report.append("  â€¢ Investigate outliers - they may be errors or important insights")
        
        return "\n".join(report)
        
    except Exception as e:
        return f"Data quality check failed: {str(e)}"


# TOOL CATEGORIES AND CAPABILITIES / å·¥å…·åˆ†ç±»å’Œèƒ½åŠ›:
# 1. INFORMATION RETRIEVAL / ä¿¡æ¯æ£€ç´¢: search_tool (web search capabilities)
# 2. CODE EXECUTION / ä»£ç æ‰§è¡Œ: python_inter (Python environment)
# 3. VISUALIZATION / å¯è§†åŒ–: fig_inter (matplotlib/seaborn plotting)
# 4. DATABASE OPERATIONS / æ•°æ®åº“æ“ä½œ: sql_inter, extract_data (MySQL integration)
# 5. DATA MANAGEMENT / æ•°æ®ç®¡ç†: export_data (multi-format export)
# 6. QUALITY ASSURANCE / è´¨é‡ä¿è¯: data_preview, data_quality_check (data validation)
# 7. EFFICIENCY TOOLS / æ•ˆç‡å·¥å…·: quick_chart (template charts), query_history (SQL management)

tools = [search_tool, python_inter, fig_inter, sql_inter, extract_data, 
         export_data, data_preview, quick_chart, query_history, data_quality_check]

model = ChatOpenAI(
    model=os.getenv('MODEL_NAME'),        
    api_key=os.getenv('OPENAI_API_KEY'),  
    temperature=0.2                       
)


graph = create_react_agent(model=model, tools=tools, prompt=prompt)
